{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDl2YT5-mGg4"
      },
      "source": [
        "![Cabec%CC%A7alho_notebook.png](cabecalho_notebook.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGG5zXirmGg8"
      },
      "source": [
        "# Classificação de Atividade Humana com PCA\n",
        "\n",
        "Vamos trabalhar com a base da demonstração feita em aula, mas vamos explorar um pouco melhor como é o desempenho da árvore variando o número de componentes principais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ttgtr07YmGg_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "filename_features = \"/content/features.txt\"\n",
        "filename_labels = \"/content/activity_labels.txt\"\n",
        "\n",
        "filename_subtrain = \"/content/subject_train.txt\"\n",
        "filename_xtrain = \"/content/X_train.txt\"\n",
        "filename_ytrain = \"/content/y_train.txt\"\n",
        "\n",
        "filename_subtest = \"/content/subject_test.txt\"\n",
        "filename_xtest = \"/content/X_test.txt\"\n",
        "filename_ytest = \"/content/y_test.txt\"\n",
        "\n",
        "# Carregar features e labels\n",
        "features = pd.read_csv(filename_features, sep='\\s+', header=None, names=['idx', 'nome_var'], encoding=\"utf-8\")\n",
        "features = features['nome_var'].tolist()\n",
        "\n",
        "# Remover duplicatas dos nomes das variáveis\n",
        "features = list(dict.fromkeys(features))  # Remove duplicatas mantendo a ordem\n",
        "\n",
        "labels = pd.read_csv(filename_labels, sep='\\s+', header=None, names=['cod_label', 'label'], encoding=\"utf-8\")\n",
        "\n",
        "# Carregar dados de treino\n",
        "subject_train = pd.read_csv(filename_subtrain, header=None, names=['subject_id'], encoding=\"utf-8\")['subject_id']\n",
        "X_train = pd.read_csv(filename_xtrain, sep='\\s+', header=None, names=features, encoding=\"utf-8\")\n",
        "y_train = pd.read_csv(filename_ytrain, header=None, names=['cod_label'], encoding=\"utf-8\")\n",
        "\n",
        "# Carregar dados de teste\n",
        "subject_test = pd.read_csv(filename_subtest, header=None, names=['subject_id'], encoding=\"utf-8\")['subject_id']\n",
        "X_test = pd.read_csv(filename_xtest, sep='\\s+', header=None, names=features, encoding=\"utf-8\")\n",
        "y_test = pd.read_csv(filename_ytest, header=None, names=['cod_label'], encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyqrQdKMmGhC"
      },
      "source": [
        "## PCA com variáveis padronizadas\n",
        "\n",
        "Reflexão sobre a escala das variáveis:\n",
        "\n",
        "**Variáveis em métricas muito diferentes** podem interferir na análise de componentes principais. Lembra que variância é informação pra nós? Pois bem, tipicamente se há uma variável monetária como salário, vai ter uma ordem de variabilidade bem maior que número de filhos, tempo de emprego ou qualquer variável dummy. Assim, as variáveis de maior variância tendem a \"dominar\" a análise. Nesses casos é comum usar a padronização das variáveis.\n",
        "\n",
        "Faça duas análises de componentes principais para a base do HAR - com e sem padronização e compare:\n",
        "\n",
        "- A variância explicada por componente\n",
        "- A variância explicada acumulada por componente\n",
        "- A variância percentual por componente\n",
        "- A variância percentual acumulada por componente\n",
        "- Quantas componentes você escolheria, em cada caso para explicar 90% da variância?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Função de padronização personalizada\n",
        "def padroniza(s):\n",
        "    if s.std() > 0:\n",
        "        s = (s - s.mean()) / s.std()\n",
        "    return s\n",
        "\n",
        "# Aplicando a padronização personalizada nos dados de treinamento\n",
        "X_train_pad = pd.DataFrame(X_train).apply(padroniza, axis=0)\n",
        "\n",
        "# PCA sem padronização (dados originais)\n",
        "pca_no_scaling = PCA()\n",
        "X_pca_no_scaling = pca_no_scaling.fit_transform(X_train)\n",
        "\n",
        "# PCA com padronização personalizada\n",
        "pca_with_custom_scaling = PCA()\n",
        "X_pca_with_custom_scaling = pca_with_custom_scaling.fit_transform(X_train_pad)\n",
        "\n",
        "# Variância explicada e acumulada - sem padronização\n",
        "explained_variance_no_scaling = pca_no_scaling.explained_variance_ratio_\n",
        "cumulative_variance_no_scaling = explained_variance_no_scaling.cumsum()\n",
        "\n",
        "# Variância explicada e acumulada - com padronização personalizada\n",
        "explained_variance_with_custom_scaling = pca_with_custom_scaling.explained_variance_ratio_\n",
        "cumulative_variance_with_custom_scaling = explained_variance_with_custom_scaling.cumsum()\n",
        "\n",
        "# Exibir os resultados em um DataFrame para comparação\n",
        "import pandas as pd\n",
        "\n",
        "resultados = pd.DataFrame({\n",
        "    \"Componente\": range(1, len(explained_variance_no_scaling) + 1),\n",
        "    \"Var. Explicada Sem Padronização\": explained_variance_no_scaling,\n",
        "    \"Var. Explicada Acumulada Sem Padronização\": cumulative_variance_no_scaling,\n",
        "    \"Var. Explicada Com Padronização Customizada\": explained_variance_with_custom_scaling,\n",
        "    \"Var. Explicada Acumulada Com Padronização Customizada\": cumulative_variance_with_custom_scaling\n",
        "})\n",
        "\n",
        "print(resultados)\n",
        "\n",
        "# Identificar o número de componentes necessários para explicar 90% da variância\n",
        "num_components_no_scaling = (cumulative_variance_no_scaling >= 0.90).argmax() + 1\n",
        "num_components_with_custom_scaling = (cumulative_variance_with_custom_scaling >= 0.90).argmax() + 1\n",
        "\n",
        "print(f\"Número de componentes sem padronização para explicar 90% da variância: {num_components_no_scaling}\")\n",
        "print(f\"Número de componentes com padronização personalizada para explicar 90% da variância: {num_components_with_custom_scaling}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYMxo8P_mr7c",
        "outputId": "3460d194-e0a5-487f-84d5-51bd96afe1eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Componente  Var. Explicada Sem Padronização  \\\n",
            "0             1                         0.679477   \n",
            "1             2                         0.049314   \n",
            "2             3                         0.021167   \n",
            "3             4                         0.017638   \n",
            "4             5                         0.013143   \n",
            "..          ...                              ...   \n",
            "472         473                         0.000000   \n",
            "473         474                         0.000000   \n",
            "474         475                         0.000000   \n",
            "475         476                         0.000000   \n",
            "476         477                         0.000000   \n",
            "\n",
            "     Var. Explicada Acumulada Sem Padronização  \\\n",
            "0                                     0.679477   \n",
            "1                                     0.728792   \n",
            "2                                     0.749959   \n",
            "3                                     0.767596   \n",
            "4                                     0.780740   \n",
            "..                                         ...   \n",
            "472                                   1.000000   \n",
            "473                                   1.000000   \n",
            "474                                   1.000000   \n",
            "475                                   1.000000   \n",
            "476                                   1.000000   \n",
            "\n",
            "     Var. Explicada Com Padronização Customizada  \\\n",
            "0                                       0.545800   \n",
            "1                                       0.062532   \n",
            "2                                       0.028253   \n",
            "3                                       0.020119   \n",
            "4                                       0.018582   \n",
            "..                                           ...   \n",
            "472                                     0.000000   \n",
            "473                                     0.000000   \n",
            "474                                     0.000000   \n",
            "475                                     0.000000   \n",
            "476                                     0.000000   \n",
            "\n",
            "     Var. Explicada Acumulada Com Padronização Customizada  \n",
            "0                                             0.545800      \n",
            "1                                             0.608332      \n",
            "2                                             0.636586      \n",
            "3                                             0.656705      \n",
            "4                                             0.675287      \n",
            "..                                                 ...      \n",
            "472                                           1.000000      \n",
            "473                                           1.000000      \n",
            "474                                           1.000000      \n",
            "475                                           1.000000      \n",
            "476                                           1.000000      \n",
            "\n",
            "[477 rows x 5 columns]\n",
            "Número de componentes sem padronização para explicar 90% da variância: 29\n",
            "Número de componentes com padronização personalizada para explicar 90% da variância: 54\n",
            "CPU times: user 2.3 s, sys: 48.3 ms, total: 2.35 s\n",
            "Wall time: 3.44 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise de Componentes em Markdown\n",
        "\n",
        "### Distribuição e Comportamento dos Dados\n",
        "- **Var. Explicada Sem Padronização:**\n",
        "  - A maior parte da variância é explicada pelo primeiro componente (**67,95%**), com uma queda significativa nos componentes subsequentes.\n",
        "  - Após os primeiros 5 componentes, a contribuição para a variância se torna bastante pequena.\n",
        "\n",
        "- **Var. Explicada Acumulada Sem Padronização:**\n",
        "  - Os primeiros **29 componentes** explicam **90%** da variância.\n",
        "  - Sem padronização, poucos componentes são necessários para capturar a maioria da variância dos dados.\n",
        "\n",
        "---\n",
        "\n",
        "### Impacto da Padronização Customizada\n",
        "- **Var. Explicada Com Padronização Customizada:**\n",
        "  - O primeiro componente explica **54,58%** da variância, seguido por uma redução gradual nos componentes subsequentes.\n",
        "  - Isso indica que a padronização ajustou os pesos das variáveis explicativas.\n",
        "\n",
        "- **Var. Explicada Acumulada Com Padronização Customizada:**\n",
        "  - São necessários **54 componentes** para alcançar **90%** da variância acumulada.\n",
        "  - Com padronização, mais componentes são necessários para capturar a variância dos dados.\n",
        "\n",
        "---\n",
        "\n",
        "### Resumo Comparativo\n",
        "| Método                     | Componentes para 90% da Variância | Contribuição Inicial (1º Componente) |\n",
        "|----------------------------|-----------------------------------|--------------------------------------|\n",
        "| **Sem Padronização**       | 29                               | 67,95%                              |\n",
        "| **Com Padronização**       | 54                               | 54,58%                              |\n",
        "\n",
        "---\n",
        "\n",
        "### Desempenho Computacional\n",
        "- **CPU Times:** Usuário: **2,3s** | Sistema: **48,3ms** | Total: **2,35s**\n",
        "- **Wall Time:** **3,44s**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "i5JcTFwyoRQ4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_pwQ4L4mGhD"
      },
      "source": [
        "## Árvore com PCA\n",
        "\n",
        "Faça duas uma árvore de decisão com 10 componentes principais - uma com base em dados padronizados e outra sem padronizar. Utilize o ```ccp_alpha=0.001```.\n",
        "\n",
        "Compare a acurácia na base de treino e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wENHVM4kmGhD",
        "outputId": "78028c1b-6ce8-4137-8c30-c6e12bd91fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia sem padronização: 0.7848659653885307\n",
            "Acurácia com padronização personalizada: 0.6308109942314218\n",
            "CPU times: user 1.64 s, sys: 11.2 ms, total: 1.65 s\n",
            "Wall time: 1.46 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Função de padronização personalizada\n",
        "def padroniza(s):\n",
        "    if s.std() > 0:\n",
        "        s = (s - s.mean()) / s.std()\n",
        "    return s\n",
        "\n",
        "# Aplicar a padronização personalizada nos dados de treinamento\n",
        "X_train_pad = pd.DataFrame(X_train).apply(padroniza, axis=0)\n",
        "\n",
        "# Aplicar a padronização personalizada nos dados de teste\n",
        "X_test_pad = pd.DataFrame(X_test).apply(padroniza, axis=0)\n",
        "\n",
        "# Reduzir dimensionalidade para 10 componentes principais - sem padronização\n",
        "pca_no_scaling = PCA(n_components=10)\n",
        "X_train_pca_no_scaling = pca_no_scaling.fit_transform(X_train)\n",
        "X_test_pca_no_scaling = pca_no_scaling.transform(X_test)\n",
        "\n",
        "# Reduzir dimensionalidade para 10 componentes principais - com padronização personalizada\n",
        "pca_with_scaling = PCA(n_components=10)\n",
        "X_train_pca_with_scaling = pca_with_scaling.fit_transform(X_train_pad)\n",
        "X_test_pca_with_scaling = pca_with_scaling.transform(X_test_pad)\n",
        "\n",
        "# Treinar a árvore de decisão - sem padronização\n",
        "tree_no_scaling = DecisionTreeClassifier(ccp_alpha=0.001, random_state=42)\n",
        "tree_no_scaling.fit(X_train_pca_no_scaling, y_train)\n",
        "y_pred_no_scaling = tree_no_scaling.predict(X_test_pca_no_scaling)\n",
        "\n",
        "# Treinar a árvore de decisão - com padronização\n",
        "tree_with_scaling = DecisionTreeClassifier(ccp_alpha=0.001, random_state=42)\n",
        "tree_with_scaling.fit(X_train_pca_with_scaling, y_train)\n",
        "y_pred_with_scaling = tree_with_scaling.predict(X_test_pca_with_scaling)\n",
        "\n",
        "# Avaliar desempenho\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(f\"Acurácia sem padronização: {accuracy_no_scaling}\")\n",
        "print(f\"Acurácia com padronização personalizada: {accuracy_with_scaling}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar a acurácia na base de treino - sem padronização\n",
        "accuracy_train_no_scaling = accuracy_score(y_train, tree_no_scaling.predict(X_train_pca_no_scaling))\n",
        "\n",
        "# Avaliar a acurácia na base de treino - com padronização personalizada\n",
        "accuracy_train_with_scaling = accuracy_score(y_train, tree_with_scaling.predict(X_train_pca_with_scaling))\n",
        "\n",
        "# Exibir os resultados de treino e teste\n",
        "print(f\"Acurácia na base de treino (sem padronização): {accuracy_train_no_scaling}\")\n",
        "print(f\"Acurácia na base de teste (sem padronização): {accuracy_no_scaling}\")\n",
        "print(f\"Acurácia na base de treino (com padronização personalizada): {accuracy_train_with_scaling}\")\n",
        "print(f\"Acurácia na base de teste (com padronização personalizada): {accuracy_with_scaling}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggYRi5QApXXz",
        "outputId": "8798fe6b-5b25-4278-d2d5-9ad073f219e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia na base de treino (sem padronização): 0.8454842219804135\n",
            "Acurácia na base de teste (sem padronização): 0.7848659653885307\n",
            "Acurácia na base de treino (com padronização personalizada): 0.7383025027203483\n",
            "Acurácia na base de teste (com padronização personalizada): 0.6308109942314218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Análise e Comparação das Acurácias\n",
        "\n",
        "Ao comparar as acurácias sem padronização e com padronização personalizada, observamos que a aplicação de padronização personalizada teve um impacto negativo nos resultados, tanto na base de treino quanto na base de teste.\n",
        "\n",
        "### **Resultados sem padronização**\n",
        "- Na **base de treino**, a acurácia foi de **0.845**. Esse valor indica um bom desempenho do modelo ao aprender os padrões dos dados durante o treinamento.\n",
        "- Na **base de teste**, a acurácia foi de **0.784**, indicando que o modelo conseguiu generalizar relativamente bem para dados novos.\n",
        "\n",
        "### **Resultados com padronização personalizada**\n",
        "- Na **base de treino**, a acurácia caiu para **0.738**. Esse declínio pode significar que a padronização personalizada interferiu na capacidade do modelo de capturar os padrões presentes nos dados de treinamento.\n",
        "- Na **base de teste**, a acurácia foi de **0.630**, uma queda considerável em comparação aos resultados sem padronização. Isso sugere que a padronização personalizada prejudicou ainda mais a capacidade de generalização do modelo.\n",
        "\n",
        "### **Comparação**\n",
        "A padronização personalizada parece ter introduzido um nível de transformação nos dados que dificultou o aprendizado do modelo e sua habilidade de generalização. Apesar de técnicas de padronização frequentemente ajudarem a melhorar o desempenho do modelo, neste caso, os resultados indicam que a abordagem personalizada utilizada não foi eficaz.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "QHF8XRLfqC9i"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Índice",
      "title_sidebar": "Conteúdo",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}